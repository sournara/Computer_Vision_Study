{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyORi7ZSqIeemDJH5ZLZvFZ1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sournara/Computer_Vision_Study/blob/main/PyTorch/PyTorch_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 파이토치(PyTorch) 개요\n",
        "- PyTorch는 기계 학습 프레임워크(framework) 중 하나이다.\n",
        "  - PyTorch의 텐서(tensor)는 NumPy 배열과 매우 유사하다.\n",
        "- PyTorch를 사용하면, GPU 연동을 통해 효율적으로 딥러닝 모델을 학습할 수 있다.\n",
        "- Google Colab을 이용하면, 손쉽게 PyTorch를 시작할 수 있다.\n",
        "- Google Colab에서는 [런타임]-[런타임 유형 변경]에서 **GPU를 선택**할 수 있다."
      ],
      "metadata": {
        "id": "ZvujMRAy5qhQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UG7g8D2J5bFe"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) GPU 사용 여부 체크하기\n",
        "- 텐서간의 연산을 수행할 때, 기본적으로 두 텐서가 같은 장치에 있어야 한다.\n",
        "- 따라서 가능하면, 연산을 수행하는 텐서들을 모두 GPU에 올린 뒤에 연산을 수행한다."
      ],
      "metadata": {
        "id": "zUyhXGgo6aw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [[1, 2], [3, 4]]\n",
        "\n",
        "x = torch.tensor(data)\n",
        "print(x.is_cuda)\n",
        "\n",
        "x = x.cuda() # GPU로 옮기기\n",
        "print(x.is_cuda)\n",
        "\n",
        "x = x.cpu() # CPU로 옮기기\n",
        "print(x.is_cuda)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaycNNFc6Vdr",
        "outputId": "de1d8e15-332d-4457-c305-68850cfde496"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 서로 다른 장치(device)에 있는 텐서끼리 연산을 수행하면 오류가 발생한다."
      ],
      "metadata": {
        "id": "Tgm-R1Mi7Kzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU 장치의 텐서\n",
        "a = torch.tensor([\n",
        "    [1, 1],\n",
        "    [2, 2]\n",
        "]).cuda()\n",
        "\n",
        "# CPU 장치의 텐서\n",
        "b = torch.tensor([\n",
        "    [5, 6],\n",
        "    [7, 8]\n",
        "])\n",
        "\n",
        "# print(torch.matmul(a, b)) # 오류 발생\n",
        "print(torch.matmul(a.cpu(), b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cpwENE767Rb",
        "outputId": "63ac854b-32cb-4507-dbd9-0c6dce1bb073"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[12, 14],\n",
            "        [24, 28]])\n"
          ]
        }
      ]
    }
  ]
}